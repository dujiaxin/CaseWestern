# -*- coding: utf-8 -*-
"""transformerXL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-F_6zGycsOZfKJLegU8tFqYgWpKLImCp
"""

!pip install transformers python-docx docx2txt natsort pandas numpy

import sys
import os
import random

import torch
import torch.nn as nn
import torch.nn.functional as F

import docx2txt as d2t
from docx import Document
import numpy as np
import pdb, traceback, sys

from transformers import TransfoXLTokenizer, TransfoXLModel

from google.colab import drive
drive.mount('/content/drive')

# After executing the cell above, Drive
# files will be present in "/content/drive/My Drive".
!ls "/content/drive/My Drive/CW/CLEANED_2"

tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')
#model = TransfoXLLMHeadModel.from_pretrained('transfo-xl-wt103')
model = TransfoXLModel.from_pretrained('transfo-xl-wt103')

mask = 500
file_counter = 0

model.output_hidden_states = True

text_filepath = '/content/drive/My Drive/CW/CLEANED_2/'

tuples = []
#all_input_files = []

for root, dirs, files in os.walk(text_filepath,topdown=True):

  if not files:
    continue

  '''GET folder name'''
  hierachy = root.split("/")
  folder = int(hierachy[-1])
  #print(files)

  for filename in files:

    #if file_counter > 100:
      #break

    tu = []

    '''filepath for each file'''
    f = root +"/"+ filename

    #print(file)
    '''read file content'''
    text_dataframe = d2t.process(f)

    if len(text_dataframe) > mask:
      text_dataframe = text_dataframe[:mask]
    elif len(text_dataframe) < mask:
      continue

    input_ids = torch.tensor(tokenizer.encode(text_dataframe)).unsqueeze(0)  # Batch size 1
    outputs = model(input_ids)

    last_hidden_states, mems, hidden_states = outputs

    #print(len(last_hidden_states))
    #print(len(last_hidden_states[0]))
    #print(len(last_hidden_states[0][0]))

    average_embedding = torch.zeros(1024, dtype=torch.float)

    word_embedding_count = 0

    for word_embedding in last_hidden_states[0]:
      average_embedding = average_embedding.add(word_embedding) 
      word_embedding_count += 1
      
    #numpy_average_embedding = average_embedding.div(word_embedding_count)
    numpy_average_embedding = average_embedding.detach().numpy()
    #numpy_average_embedding = numpy_average_embedding .div(word_embedding_count)
    
    numpy_average_embedding = numpy_average_embedding / word_embedding_count
    bias = random.randrange(0,2)

    tu.append(f)
    tu.append(numpy_average_embedding)
    tu.append(bias)

    #print(len(average_embedding))
    tuples.append(tu)

    del tu
    del bias
    del numpy_average_embedding
    del average_embedding
    del outputs
    del input_ids
    del text_dataframe

  #file_counter += 1

print(len(tuples))
print(tuples[2145])
print(3216 - 2145)

for l in tuples:
  print(l[0])
  x = np.reshape(l[0], (1024, 1))
  print(x)
  break

def load_data(data: list):

    partition = 700

    training_data = data[:partition]
    test_data = data[partition:]

    return (training_data, test_data)

def vectorized_result(j):
    """Return a 10-dimensional unit vector with a 1.0 in the jth
    position and zeroes elsewhere.  This is used to convert a digit
    (0...9) into a corresponding desired output from the neural
    network."""
    j = int(j)
    e = np.zeros((2, 1))
    e[j] = 1.0
    return e

def load_data_wrapper(tuples):
    """Return a tuple containing ``(training_data, validation_data,
    test_data)``. Based on ``load_data``, but the format is more
    convenient for use in our implementation of neural networks.
    In particular, ``training_data`` is a list containing 50,000
    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray
    containing the input image.  ``y`` is a 10-dimensional
    numpy.ndarray representing the unit vector corresponding to the
    correct digit for ``x``.
    ``validation_data`` and ``test_data`` are lists containing 10,000
    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional
    numpy.ndarry containing the input image, and ``y`` is the
    corresponding classification, i.e., the digit values (integers)
    corresponding to ``x``.
    Obviously, this means we're using slightly different formats for
    the training data and the validation / test data.  These formats
    turn out to be the most convenient for use in our neural network
    code."""
    
    tr_d, te_d = load_data(tuples)

    training_inputs = [np.reshape(x[0], (1024, 1)) for x in tr_d]     
    training_results = [vectorized_result(y[1]) for y in tr_d]
    training_data = zip(training_inputs, training_results)
    
    
    test_inputs = [np.reshape(x[0], (1024, 1)) for x in te_d]
    test_results = [y[1] for y in te_d]
    test_data = zip(test_inputs, test_results)
    
    return (training_data, test_data)
    #return training_data

def sigmoid(z):
    '''
    To do the sigmoid functiopn which has the equationï¼š

                        1
    sigmoid_value = ----------
                    1 + e ^ -x
    '''
    return 1.0/(1.0+np.exp(-z))

def sigmoid_prime(z):
    """Derivative of the sigmoid function."""
    return sigmoid(z)*(1-sigmoid(z))

def cost_derivative(output_activations, y):
    """
    Return the vector of partial derivatives partial C_x partial a for the output activations.
    """
    return output_activations-y

def normalize_list_numpy(arr):
    normalized_list = arr / np.linalg.norm(arr)
    return normalized_list

class Network(object):

    def __init__(self, sizes):
        '''
        sizes:
            Layers and its neurals in each layer. The input format should be a list.
            i.e. "net = Network([2, 3, 1])"

            input layer        hidden layer 1        output layer

            n1                      n3

                                    n4                    n6

            n2                      n5

        num_layer: 
            The total layers this network has(include input layer, hidden layer, output layer)

        biases:
            The bias for each neural in each layer(except input layer). Generated via Numpy array.
            i.e.

            biases = [ 

                #All biases for {n3, n4, n5}
                [   [n3],
                    [n4],
                    [n5]  ],

                #All biases for {n6}
                [   [n6]   ]
            ]

        weights:
            The weight for neural in one layer to neural in next layer(except output layer). Generated Via Numpy array.
            i.e.

            weights = [ 

                #All weights for {n1, n2} to {n3, n4, n5}
                [   [n1-n3, n2-n3],
                    [n1-n4, n2-n4],
                    [n1-n5, n2-n5]  ],

                #All weights for {n3, n4, n5} to {n6}
                [   [n3-n6, n4-n6, n5-n6]   ]
            ]
        '''
        self.num_layers = len(sizes)
        self.sizes = sizes
        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]
        self.weights = [np.random.randn(y, x) 
                        for x, y in zip(sizes[:-1], sizes[1:])]
    
    def feedforward(self, a):
        """Return the output of the network if "a" is input."""
        for b, w in zip(self.biases, self.weights):
            a = sigmoid(np.dot(w, a)+b)
        return a
    
    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):
        """
        Train the neural network using mini-batch stochastic
        gradient descent.  
        
        training_data -  a list of tuples "(input data, expected outcome)" <=> "(x, y)" 
        representing the training inputs and the desired outputs.  
        
        epoch - numbers of training round. The more epoch used, the more accuracy you get. 
        (Just therotically, because sometimes it will reach the bottleneck of subtle error)
        
        mini_batch_size - a size you decided for every mini_batcha.  
        
        test_data - if it is provided then the network will be evaluated against the test data after each
        epoch, and partial progress printed out.  
        This is useful for tracking progress, but slows things down substantially.
        """
        if test_data is not None:
            test_data = list(test_data)
            n_test = len(test_data)
        
        training_data = list(training_data)
        n = len(training_data)
        
        """
        Do the divide-action for training data to mini_batches.
        Every of them is a sub-list of tuples "(input data, expected outcome)" <=> "(x, y)
        """
        for j in range(epochs):
            random.shuffle(training_data)
            mini_batches = [
                training_data[k:k+mini_batch_size]
                for k in range(0, n, mini_batch_size)
            ]
            
            for mini_batch in mini_batches:
                self.update_mini_batch(mini_batch, eta)
                
            """
            This part is evaluate accuracy via using testing data to against the trained network 
            """
            if test_data is not None :
                print ("Epoch {0}: {1} / {2}".format(j, self.evaluate(test_data), n_test))
            else:
                print ("Epoch {0} complete".format(j))
                
    def update_mini_batch(self, mini_batch, eta):
        """
        Update the network's weights and biases by applying
        gradient descent using backpropagation to a single mini batch.
        
        mini_batch - a list of tuples "(input data, expected outcome)" <=> "(x, y)" 
        
        eta -learning rate.
        """
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]
        
        """
        calculate the gradient vector for every mini batch and add them up
        """
        for x, y in mini_batch:
            """
            Use different feedforward and backword, since now using the stochastic gradient descent
            """
            activations, zs = self.feedforword_stochastic(x, y)
            delta_nabla_b, delta_nabla_w = self.backprop_stochastic(y, activations, zs)
            
            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]
            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]
        
        """
        Updates the final weights and biases based on every mini batch 
        """    
        self.weights = [w-(eta/len(mini_batch))*nw 
                        for w, nw in zip(self.weights, nabla_w)]
        self.biases = [b-(eta/len(mini_batch))*nb 
                       for b, nb in zip(self.biases, nabla_b)]
        
    def feedforword_stochastic(self,x,y):
        activation = x
        activations = [x] # list to store all the activations, layer by layer
        zs = [] # list to store all the z vectors, layer by layer
        for b, w in zip(self.biases, self.weights):
            #print("a is:")
            #print(activation)
            #print()
            #print("w is:")
            #print(w)
            #print()
            #print("b is:")
            #print(b)
            #print()
            z = np.dot(w, activation)+b
            zs.append(z)
            activation = sigmoid(z)
            activations.append(activation)
            
        return (activations, zs)
        
    def backprop_stochastic(self, y, activations, zs):
        """
        Return a tuple `(nabla_b, nabla_w)` representing the
        11gradient for the cost function C_x.  `nabla_b` and
        `nabla_w` are layer-by-layer lists of numpy arrays, similar
        to `self.biases` and `self.weights`.
        """
        nabla_b = [np.zeros(b.shape) for b in self.biases]
        nabla_w = [np.zeros(w.shape) for w in self.weights]

        """backward pass"""
        delta = cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])
        nabla_b[-1] = delta
        nabla_w[-1] = np.dot(delta, activations[-2].transpose())
        
        """ 
        Note that the variable l in the loop below is used a little
        differently to the notation in Chapter 2 of the book.  Here,
        l = 1 means the last layer of neurons, l = 2 is the
        second-last layer, and so on.  It's a renumbering of the
        scheme in the book, used here to take advantage of the fact
        that Python can use negative indices in lists.
        """
        for l in range(2, self.num_layers):
            #z = zs[-l]
            #sp = sigmoid_prime(z)
            delta = np.dot(self.weights[-l+1].transpose(), delta) * sigmoid_prime(zs[-l])
            nabla_b[-l] = delta
            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
        return (nabla_b, nabla_w)

    def evaluate(self, test_data):
        """
        Return the number of test inputs for which the neural
        network outputs the correct result. Note that the neural
        network's output is assumed to be the index of whichever
        neuron in the final layer has the highest activation.
        """
        test_results = [(np.argmax(self.feedforward(x)), y)
                        for (x, y) in test_data]
        return sum(int(x == y) for (x, y) in test_results)

try:
  training_collection, test_collection = load_data_wrapper(tuples)
except:
  extype, value, tb = sys.exc_info()
  traceback.print_exc()
  pdb.post_mortem(tb)



"""initial network"""
net = Network([1024, 512, 256, 128, 64, 32, 16, 8, 4, 2])

net.SGD(training_collection, 30, 4, 0.1, test_data=test_collection)